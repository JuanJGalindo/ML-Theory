{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanJGalindo/ML-Theory/blob/main/Parcial1/Parcial_1.123_TAM_2025_2_Juan_Jose_Galindo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Punto 1 - Solución y Problema de Optimización para Distintos Enfoques del Modelo de Regresión con Ruido Gaussiano**\n",
        "\n",
        "**Notación**:\n",
        "\n",
        "Sea $\\Phi\\in\\mathbb{R}^{N\\times P}$ la matriz de diseño cuyas filas son $\\phi(x_n)^\\top$.\n",
        "Denotamos $\\mathbf t=[t_1,\\dots,t_N]^\\top$. Además, $\\|v\\|^2=v^\\top v.$\n",
        "\n",
        "---\n",
        "\n",
        "###**1. Mínimos Cuadrados (Least Squares / LS)**\n",
        "\n",
        "**Objetivo:**\n",
        "\n",
        "Buscamos encontrar los pesos $w$, tales que estos minimicen el error cuadrático residual:\n",
        "$$\n",
        "\\mathcal{L}(w)= \\mathrm{RSS}(w)=\\sum_{n=1}^N (t_n-\\phi(x_n)^\\top w)^2\n",
        "= \\|\\mathbf t - \\Phi^\\top w\\|^2 .\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_{LS}= \\arg\\min_w \\mathrm{RSS}(w)= \\arg\\min_w\\sum_{n=1}^N (t_n-\\phi(x_n)^\\top w)^2\n",
        "= \\arg\\min_w\\|\\mathbf t - \\Phi^\\top w\\|^2 .\n",
        "$$\n",
        "\n",
        "Así, para minimizar RSS, escribimos en forma matricial y derivamos:\n",
        "$$\n",
        "\\mathcal{L}(w)=(\\mathbf t-\\Phi w)^\\top(\\mathbf t-\\Phi w)\n",
        "= \\mathbf t^\\top \\mathbf t - 2 w^\\top \\Phi^\\top \\mathbf t + w^\\top \\Phi^\\top\\Phi w.\n",
        "$$\n",
        "\n",
        "Calculamos el gradiente respecto a $w$:\n",
        "$$\n",
        "\\nabla_w \\mathcal{L}(w) = -2\\Phi^\\top \\mathbf t + 2\\Phi^\\top\\Phi\\, w.\n",
        "$$\n",
        "\n",
        "Y anulamos el gradiente (condición de óptimo - igual a cero):\n",
        "$$\n",
        "\\Phi^\\top\\Phi\\, w = \\Phi^\\top \\mathbf t.\n",
        "$$\n",
        "\n",
        "Suponiendo $\\Phi^\\top\\Phi$ invertible, la solución analítica es:\n",
        "$$\n",
        "\\boxed{\\,w_{LS}=(\\Phi^\\top\\Phi)^{-1}\\Phi^\\top \\mathbf t\\,}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "###**2. Mínimos Cuadrados Regularizados (Ridge / RLS)**\n",
        "**Objetivo:**\n",
        "\n",
        "Regularizar mínimos cuadrados para controlar complejidad y asegurar invertibilidad de la matriz $\\Phi^\\top\\Phi$:\n",
        "$$\n",
        "\\mathcal{L}_\\lambda(w)=\\sum_{n=1}^N (t_n-\\phi(x_n)^\\top w)^2 + \\lambda \\|w\\|^2\n",
        "= \\|\\mathbf t - \\Phi^\\top w\\|^2 + \\lambda w^\\top w,\n",
        "\\qquad \\lambda>0.\n",
        "$$\n",
        "\n",
        "Donde $\\lambda$ y $\\|w\\|^2$ son el parámetro de regularización y la norma L2 de los pesos del modelo, respectivamente.\n",
        "\n",
        "Así, derivamos:\n",
        "$$\n",
        "\\nabla_w J_\\lambda(w) = -2\\Phi^\\top\\mathbf t + 2\\Phi^\\top\\Phi\\, w + 2\\lambda w\n",
        "= 2(\\Phi^\\top\\Phi + \\lambda I) w - 2\\Phi^\\top\\mathbf t.\n",
        "$$\n",
        "\n",
        "Anulando el gradiente:\n",
        "$$\n",
        "(\\Phi^\\top\\Phi + \\lambda I) w = \\Phi^\\top\\mathbf t.\n",
        "$$\n",
        "\n",
        "Como $\\Phi^\\top\\Phi + \\lambda I$ es invertible para $\\lambda>0$, la solución es:\n",
        "$$\n",
        "\\boxed{\\,w_{RLS}=(\\Phi^\\top\\Phi + \\lambda I)^{-1}\\Phi^\\top \\mathbf t \\,}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "###**3. Máxima Verosimilitud (MLE) bajo ruido Gaussiano**\n",
        "**Objetivo:**\n",
        "\n",
        "Buscamos encontrar los pesos $w$ tales que permitan maximizar la probabilidad de obtener el conjunto de datos dados ellos mismos y el parámetro de ruido $\\sigma_\\eta^2$:\n",
        "$$\n",
        "\\arg\\max_{w, \\sigma_\\eta^2}p(\\mathbf t\\mid \\Phi,w,\\sigma_\\eta^2)=\\arg\\max_{w, \\sigma_\\eta^2}\\prod_{n=1}^N \\mathcal{N}(t_n\\mid \\phi(x_n)^\\top w,\\sigma_\\eta^2 I)\n",
        "$$\n",
        "\n",
        "Considerando datos independientes e idénticamente distribuidos, podemos considerar la siguiente función de costo (Log-Verosimilitud ó Log-Likelihood):\n",
        "$$\n",
        "\\log p(\\mathbf t\\mid \\Phi,w,\\sigma_\\eta^2)\n",
        "= \\sum_{n=1}^{N} (log\\,\\mathcal{N}(t_n|\\phi(x_n)^\\top w , \\sigma^2I)) = -\\frac{N}{2}\\log(2\\pi\\sigma_\\eta^2) - \\frac{1}{2\\sigma_\\eta^2}\\|\\mathbf t - \\Phi^\\top w\\|^2.\n",
        "$$\n",
        "\n",
        "Maximizando la log-verosimilitud respecto a $w$ es equivalente a minimizar $\\|\\mathbf t-\\Phi w\\|^2$ (el factor $1/(2\\sigma_\\eta^2)$ es positivo y constante con respecto a $w$). Por tanto el estimador MLE cumple:\n",
        "$$\n",
        "w_{MLE}=\\arg\\max_w \\log p(\\mathbf t\\mid \\Phi,w)\n",
        "= \\arg\\min_w \\|\\mathbf t - \\Phi^\\top w\\|^2,\n",
        "$$\n",
        "De donde se recupera la solución de mínimos cuadrados:\n",
        "$$\n",
        "\\boxed{\\,w_{MLE}=w_{LS}=(\\Phi^\\top\\Phi)^{-1}\\Phi^\\top \\mathbf t \\,.}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "###**4. Máximo A Posteriori (MAP) — Relación Bayesiana y Ridge**\n",
        "\n",
        "Se introduce el concepto de *prior*, que representa el conocimiento previo o bien, las hipótesis que se tienen respecto a la forma de la distribución de los datos a predecir. Considerando un prior gaussiano sobre $w$:\n",
        "$$\n",
        "p(w\\mid \\lambda)=\\mathcal{N}(w\\mid 0,\\lambda^{-1} I),\n",
        "$$\n",
        "donde $\\lambda$ es la precisión y codifica la varianza del prior. Con la verosimilitud gaussiana del apartado anterior, la posterior es:\n",
        "$$\n",
        "p(w\\mid \\mathbf t,\\Phi,\\sigma_\\eta^2,\\lambda)\n",
        "\\propto p(\\mathbf t\\mid \\Phi,w,\\sigma_\\eta^2)\\, p(w\\mid\\lambda).\n",
        "$$\n",
        "\n",
        "Tomando log-posterior:\n",
        "$$\n",
        "\\log p(w\\mid \\mathbf t,\\Phi)\n",
        "= -\\frac{1}{2\\sigma_\\eta^2}\\|\\mathbf t-\\Phi w\\|^2 -\\frac{\\lambda}{2}\\|w\\|^2 + \\text{const}.\n",
        "$$\n",
        "\n",
        "Maximizar la posterior (MAP) equivale a minimizar la función de costo:\n",
        "$$\n",
        "\\mathcal{L}_{MAP}(w) \\;=\\; \\frac{1}{2\\sigma_\\eta^2}\\|\\mathbf t-\\Phi w\\|^2 + \\frac{\\lambda}{2}\\|w\\|^2.\n",
        "$$\n",
        "\n",
        "Multiplicando por $2\\sigma_\\eta^2$ (constante positiva), obtenemos exactamente el problema de Ridge con $\\tilde\\lambda=\\lambda\\sigma_\\eta^2$. Derivando y anulando:\n",
        "$$\n",
        "(\\Phi^\\top\\Phi + \\lambda\\sigma_\\eta^2 I)w = \\Phi^\\top \\mathbf t\n",
        "$$\n",
        "Si reparametrizamos $\\lambda'=\\lambda\\sigma_\\eta^2$, se recupera la forma anterior; por convención frecuentemente se escribe:\n",
        "$$\n",
        "\\boxed{\\,w_{MAP}=(\\Phi^\\top\\Phi + \\lambda' I)^{-1}\\Phi^\\top \\mathbf t\\, ,}\n",
        "$$\n",
        "Mostrando que el estimador MAP con prior Gaussiano es algebraicamente equivalente a la solución de mínimos cuadrados con regularización Ridge (RLS).\n",
        "\n",
        "---\n",
        "\n",
        "###**5. Regresión Lineal Bayesiana**\n",
        "Definimos la precisión del ruido $\\beta = 1/\\sigma_\\eta^2$.\n",
        "\n",
        "**Prior y verosimilitud:**\n",
        "\n",
        "Elegimos un prior gaussiano conjugado sobre los pesos:\n",
        "$$\n",
        "p(w) = \\mathcal{N}(w\\mid 0, \\lambda^{-1} I),\n",
        "\\qquad \\lambda>0 \\text{ (prior precision)}.\n",
        "$$\n",
        "La verosimilitud de los datos dado $w$ es:\n",
        "$$\n",
        "p(\\mathbf t\\mid w) = \\mathcal{N}(\\mathbf t \\mid \\Phi w, \\beta^{-1} I).\n",
        "$$\n",
        "\n",
        "**Posterior $p(w\\mid \\mathbf t)$:**\n",
        "\n",
        "Por conjugación, la posterior es gaussiana. Calculamos su precisión y media:\n",
        "\n",
        "- Desarrollando log-posterior:\n",
        "$$\n",
        "\\log p(w\\mid \\mathbf t)\n",
        "\\propto\n",
        "\\log p(\\mathbf t\\mid w) + \\log p(w)\n",
        "= -\\frac{\\beta}{2}\\|\\mathbf t - \\Phi w\\|^2 - \\frac{\\lambda}{2}\\|w\\|^2\n",
        "$$\n",
        "\n",
        "- Expandiendo y reordenando los términos cuadráticos en $w$:\n",
        "$$\n",
        "-\\frac{\\beta}{2}(\\mathbf t^\\top\\mathbf t - 2 w^\\top \\Phi^\\top \\mathbf t + w^\\top \\Phi^\\top\\Phi w)\n",
        "- \\frac{\\lambda}{2} w^\\top w\n",
        "$$\n",
        "\n",
        "Los términos cuadráticos en $w$ definen la precisión posterior:\n",
        "$$\n",
        "S_N^{-1} = \\beta \\Phi^\\top\\Phi + \\lambda I\n",
        "$$\n",
        "La media posterior $m_N$ satisface\n",
        "$$\n",
        "S_N^{-1} m_N = \\beta \\Phi^\\top \\mathbf t,\n",
        "$$\n",
        "Es decir:\n",
        "$$\n",
        "\\boxed{\\,m_N = \\beta \\, S_N \\, \\Phi^\\top \\mathbf t, \\qquad\n",
        "S_N = (\\beta \\Phi^\\top\\Phi + \\lambda I)^{-1}\\, .\\,}\n",
        "$$\n",
        "\n",
        "Observemos que si se toma el límite $\\lambda\\to 0$ y $\\beta$ fijo, $m_N$ tiende a la solución de mínimos cuadrados ordinarios (si $\\Phi^\\top\\Phi$ invertible).\n",
        "\n",
        "**Predictiva para un nuevo $x_*$:**\n",
        "\n",
        "Sea $\\phi_*=\\phi(x_*)$. La distribución predictiva marginal de $t_*$ integrando $w$ es:\n",
        "$$\n",
        "p(t_* \\mid \\mathbf t, x_*)\n",
        "= \\int p(t_* \\mid w, x_*)\\, p(w\\mid \\mathbf t)\\, dw.\n",
        "$$\n",
        "Con $p(t_* \\mid w, x_*) = \\mathcal{N}(\\phi_*^\\top w, \\beta^{-1})$ y $p(w\\mid\\mathbf t)=\\mathcal{N}(m_N,S_N)$, se obtiene (propiedades de gaussianas):\n",
        "$$\n",
        "p(t_* \\mid \\mathbf t, x_*) = \\mathcal{N}\\big( \\, \\phi_*^\\top m_N,\\; \\beta^{-1} + \\phi_*^\\top S_N \\phi_* \\,\\big).\n",
        "$$\n",
        "La media predictiva es $\\phi_*^\\top m_N$ y la varianza predictiva tiene el término de ruido $\\beta^{-1}$ más la incertidumbre por $w$.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Regresión Kernel Rídiga — Forma dual y Conexión con el Espacio de Funciones**\n",
        "\n",
        "Sea la **solución primaria**:\n",
        "\n",
        "$$\n",
        "\\boxed{ w = (\\phi^\\top \\phi + \\lambda I_D)^{-1} \\phi^\\top y. } \\tag{1}\n",
        "$$\n",
        "\n",
        "*Welling* cita la siguiente **identidad matricial** (2):\n",
        "\n",
        "$$\n",
        "\\boxed{(P^{-1} + B^\\top R^{-1} B)^{-1} B^\\top R^{-1}\n",
        "= P B^\\top (B P B^\\top + R)^{-1}}\n",
        "\\tag{2}\n",
        "$$\n",
        "\n",
        "Utilizaremos la identidad (2) para transformar la inversa de dimensión $D \\times D$, en una inversa de dimensión $ N \\times N $.\n",
        "\n",
        "Escogemos:\n",
        "$$\n",
        "B = \\Phi, \\qquad R = I_N, \\qquad P^{-1} = \\lambda I_D \\Rightarrow P = \\lambda^{-1} I_D.\n",
        "$$\n",
        "\n",
        "Sustituimos $P = \\lambda^{-1} I_D$, $B = \\Phi$, y $R = I_N$.\n",
        "\n",
        "**Lado izquierdo de (2):**\n",
        "\n",
        "$$\n",
        "(P^{-1} + B^\\top R^{-1} B)^{-1} B^\\top R^{-1}\n",
        "= (\\lambda I_D + \\Phi \\Phi^\\top)^{-1} \\Phi.\n",
        "$$\n",
        "\n",
        "**Lado derecho de (2):**\n",
        "\n",
        "$$\n",
        "P B^\\top (B P B^\\top + R)^{-1}\n",
        "= \\lambda^{-1} I_D \\Phi^\\top (\\Phi (\\lambda^{-1} I_D) \\Phi^\\top + I_N)^{-1}.\n",
        "$$\n",
        "\n",
        "Veamos que el término inicial es:\n",
        "$$\n",
        "\\Phi (\\lambda^{-1} I_D) \\Phi^\\top + I_N\n",
        "= \\lambda^{-1} (\\Phi \\Phi^\\top + \\lambda I_N),\n",
        "$$\n",
        "Por lo que su inversa es:\n",
        "\n",
        "$$\n",
        "(\\Phi (\\lambda^{-1} I_D) \\Phi^\\top + I_N)^{-1}\n",
        "= \\lambda (\\Phi \\Phi^\\top + \\lambda I_N)^{-1}.\n",
        "$$\n",
        "\n",
        "Sustituyendo de nuevo en el lado derecho:\n",
        "\n",
        "$$\n",
        "P B^\\top (B P B^\\top + R)^{-1}\n",
        "= \\lambda^{-1} \\Phi^\\top \\big[ \\lambda (\\Phi \\Phi^\\top + \\lambda I_N)^{-1} \\big]\n",
        "= \\Phi^\\top (\\Phi \\Phi^\\top + \\lambda I_N)^{-1}.\n",
        "$$\n",
        "\n",
        "Por tanto, la identidad (2) verifica la igualdad:\n",
        "\n",
        "$$\n",
        "(\\lambda I_D + \\Phi \\Phi^\\top)^{-1} \\Phi\n",
        "= \\Phi^\\top (\\Phi \\Phi^\\top + \\lambda I_N)^{-1}.\n",
        "$$\n",
        "\n",
        "\n",
        "Comenzamos con la forma primaria **(A)**:\n",
        "\n",
        "$$\n",
        "w = (\\Phi^\\top \\Phi + \\lambda I_D)^{-1} \\Phi^\\top y.\n",
        "$$\n",
        "\n",
        "Aplicando la igualdad anterior:\n",
        "\n",
        "$$\n",
        "w = \\Phi^\\top (\\Phi \\Phi^\\top + \\lambda I_N)^{-1} y.\n",
        "$$\n",
        "\n",
        "Definimos entonces:\n",
        "\n",
        "$$\n",
        "\\alpha := (\\Phi \\Phi^\\top + \\lambda I_N)^{-1} y,\n",
        "$$\n",
        "por lo que la **representación dual (B)** es:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "w = \\Phi \\alpha, \\qquad\n",
        "\\alpha = (K + \\lambda I_N)^{-1} y.\n",
        "}\n",
        "$$\n",
        "\n",
        "Esto demuestra que la forma (A), el problema Ridge primario, y la forma (B), la representación dual en términos del kernel, son algebraicamente equivalentes, y que la **identidad (2)** es el paso clave que permite mover la inversión entre los espacios de características y de muestras.\n",
        "\n",
        "\n",
        "**NOTA:**\n",
        "\n",
        "La regresión rígida kernel es el homólogo a la regresión rígida por mínimos cuadrados en un espacio de características de mayor dimensión que nace a partir del Kernel Trick. Así, si bien en el espacio de características original es posible que no se tengan relaciones lineales, el mapeado a un nuevo espacio puede proveer de relaciones de este tipo.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Procesos Gaussianos y su equivalencia con Kernel Ridge**\n",
        "\n",
        "**Definición y Marginal de la Salida:**\n",
        "\n",
        "Un proceso gaussiano prior sobre funciones es:\n",
        "$$\n",
        "f(x) \\sim \\mathcal{GP}(0, k(x,x')).\n",
        "$$\n",
        "Sea $\\mathbf f=[f(x_1),\\dots,f(x_N)]^\\top$. Por definición,\n",
        "$$\n",
        "\\mathbf f \\sim \\mathcal{N}(0, K),\n",
        "$$\n",
        "donde $K\\in\\mathbb{R}^{N\\times N}$ con $K_{ij}=k(x_i,x_j)$. Observamos ruidos gaussianos:\n",
        "$$\n",
        "\\mathbf t = \\mathbf f + \\boldsymbol{\\eta},\\qquad \\boldsymbol{\\eta}\\sim\\mathcal{N}(0,\\sigma_\\eta^2 I).\n",
        "$$\n",
        "Entonces la marginal sobre las observaciones es:\n",
        "$$\n",
        "p(\\mathbf t \\mid X) = \\mathcal{N}\\big(0,\\; K + \\sigma_\\eta^2 I\\big).\n",
        "$$\n",
        "\n",
        "**Predicción en un nuevo punto $x_*$:**\n",
        "\n",
        "Consideramos la distribución conjunta de $[\\mathbf t; f_*]$:\n",
        "$$\n",
        "\\begin{pmatrix} \\mathbf t \\\\ f_* \\end{pmatrix}\n",
        "\\sim\n",
        "\\mathcal{N}\\left(\n",
        "0,\\;\n",
        "\\begin{pmatrix}\n",
        "K + \\sigma_\\eta^2 I & k_* \\\\\n",
        "k_*^\\top & k_{**}\n",
        "\\end{pmatrix}\n",
        "\\right),\n",
        "$$\n",
        "donde $k_* = [k(x_*,x_1),\\dots,k(x_*,x_N)]^\\top$ y $k_{**} = k(x_*,x_*)$.\n",
        "\n",
        "La condición gaussiana nos da la distribución posterior predictiva del valor latente $f_*$:\n",
        "$$\n",
        "p(f_* \\mid \\mathbf t) = \\mathcal{N}\\big( m_*,\\; v_* \\big)\n",
        "$$\n",
        "con\n",
        "$$\n",
        "m_* = k_*^\\top (K + \\sigma_\\eta^2 I)^{-1} \\mathbf t,\n",
        "\\qquad\n",
        "v_* = k_{**} - k_*^\\top (K + \\sigma_\\eta^2 I)^{-1} k_*.\n",
        "$$\n",
        "Si deseamos la distribución de la observación $t_*$, incluyendo ruido, entonces:\n",
        "$$\n",
        "p(t_* \\mid \\mathbf t) = \\mathcal{N}( m_*,\\; v_* + \\sigma_\\eta^2 ).\n",
        "$$\n",
        "\n",
        "**Equivalencia entre Kernel Ridge y GP (media posterior):**\n",
        "\n",
        "Comparando la media predictiva de GP con la predicción de KRR obtenemos:\n",
        "$$\n",
        "m_*^{\\text{GP}} = k_*^\\top (K + \\sigma_\\eta^2 I)^{-1} \\mathbf t,\n",
        "\\qquad\n",
        "\\hat t_*^{\\text{KRR}} = k_*^\\top (K + \\lambda I)^{-1} \\mathbf t.\n",
        "$$\n",
        "Por lo tanto, si identificamos $\\lambda = \\sigma_\\eta^2$, la *media* del GP posterior coincide con la predicción de Kernel Ridge. Esto muestra la fuerte relación entre la vista de pesos (Ridge) y la vista de funciones (GP), donde KRR corresponde a la estimación MAP mientras que GP proporciona además, la varianza posterior (incertidumbre) explícita, semejante al modelo bayesiano lineal gaussiano:\n",
        "$$\n",
        "\\text{GP: } \\text{media} = \\text{KRR media}, \\quad\n",
        "\\text{GP: } \\text{varianza} = k_{**} - k_*^\\top (K + \\sigma_\\eta^2 I)^{-1} k_* .\n",
        "$$\n",
        "\n",
        "\n",
        "**Verosimilitud Marginal (evidencia) sobre el espacio de pesos:**\n",
        "\n",
        "Integrando $w$ bajo el prior gaussiano obtenemos:\n",
        "$$\n",
        "p(\\mathbf t \\mid \\Phi, \\lambda, \\beta)\n",
        "= \\int p(\\mathbf t \\mid w, \\beta)\\, p(w\\mid\\lambda)\\, dw\n",
        "= \\mathcal{N}\\big(\\mathbf t \\mid 0,\\; \\beta^{-1} I + \\lambda^{-1} \\Phi \\Phi^\\top \\big).\n",
        "$$\n",
        "Definiendo $C = \\beta^{-1} I + \\lambda^{-1} \\Phi\\Phi^\\top$, la log-evidencia es:\n",
        "$$\n",
        "\\log p(\\mathbf t) = -\\frac{1}{2}\\big( \\mathbf t^\\top C^{-1}\\mathbf t + \\log\\det C + N\\log(2\\pi) \\big).\n",
        "$$\n",
        "Esta expresión es análoga a la log-verosimilitud marginal negativa (NMLL) del GP si identificamos $K=\\Phi\\Phi^\\top$ y la relación entre $\\lambda$ y $\\beta$.\n",
        "\n",
        "**Uso práctico:**\n",
        "\n",
        "Maximizar la verosimilitud marginal respecto a hiperparámetros ($\\lambda,\\beta$ o parámetros del kernel en GP) permite seleccionar regularización y parámetros de kernel de forma automática (maximización de la evidencia). En GPs se optimiza la NMLL numéricamente (gradiente) para estimar los hiperparámetros del kernel.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Comentarios finales y consideraciones prácticas - Preámbulo Punto 2**\n",
        "\n",
        "La *vista de pesos* (weight-space) y la *vista de funciones* (function-space) son dos caras de la misma moneda cuando el kernel es representable por un producto interno $\\phi(x)^\\top\\phi(x')$.\n",
        "\n",
        "La diferencia conceptual es que GP trata funciones completas y proporciona incertidumbre sobre la función, mientras que KRR proporciona una estimación puntual (que coincide con la media GP) a partir de un criterio regularizado.\n",
        "\n",
        "**Escalabilidad:**\n",
        "  - Primal (resolución en $P$): buena si **P** es pequeño y **N** grande (Inversión de $P\\times P$).\n",
        "  - Dual/kernel (resolución en $N$): costosa para **N** grande (Inversión de $N\\times N$ con $O(N^3)$).\n",
        "  - Para **N** grandes, típicamente se usan aproximaciones: *Nyström*, *Random Fourier Features*, métodos de puntos inducidos (SVGP en GPyTorch), o factorizaciones iterativas."
      ],
      "metadata": {
        "id": "8AWj08S_xpBY"
      },
      "id": "8AWj08S_xpBY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czPQCoaztvaf"
      },
      "source": [
        "# **Punto 2 - Cuadro Comparativo de Modelos de Regresión Seleccionados**\n",
        "\n",
        "| Modelo | Modelo Matemático | Función de Costo | Estrategia de Optimización | Relación con Regresión Clásica | Escalabilidad |\n",
        "|--------|-------------------|--------------------------------------|----------------------------|-------------------------------|---------------|\n",
        "| **LinearRegression** | $y = Xw + \\varepsilon$ | $ \\mathcal{L}(w)=\\frac{1}{2N}\\|y-Xw\\|^2$| <br> **Descenso del Gradiente (1)** <br> $\n",
        "w^* = (X^\\top X)^{-1} X^\\top y$ **(2)** | Modelo de Mínimos Cuadrados | Aceptable $(O(NP^2))$, <br>requiere inversa de matriz de relativamente baja dimensionalidad.<br> Puede aproximarse por gradiente descendiente, <br>lo cual la convierte en un algoritmo altamente escalable |\n",
        "| **Lasso** | $y=Xw+\\varepsilon$ | $\\mathcal{L}(w)=\\frac{1}{2N}|y-Xw\\|^2+\\alpha\\ |w|_1$ | **Descenso Coordenado:** <br>$w_{new} \\leftarrow w_{old} - \\eta \\nabla_w G(w)$ | Modelo de Mínimos Cuadrados <br>Regularizados por Norma L1 | Buena para alta **D**, lenta en **N** muy grande. <br>Puede usarse método mini-batch <br>para sortear el problema de grandes muestras |\n",
        "| **ElasticNet** | $y = Xw + \\varepsilon$ | $\\mathcal{L}(w)=\\frac{1}{2N}\\|y - Xw\\|^2 + \\alpha(\\rho\\|w\\|_1 + (1-\\rho)\\|w\\|_2^2)$ | **Descenso Coordenado Generalizado** | Modelo de Mínimos Cuadrados <br>Regularizados por Norma L1 y L2 | Alta escalabilidad, <br>algoritmo paralelizable e iterativo |\n",
        "| **KernelRidge** | $y = K\\alpha + \\varepsilon$ | $\\mathcal{L}(\\alpha)=\\|y - K\\alpha\\|^2 + \\lambda\\|\\alpha\\|^2$ | $\\alpha^*=(K+\\lambda I)^{-1}y$ | Regresión Rígida Kernel | Limitado por $(O(N^3))$, <br>recreación e inversión de matriz kernel |\n",
        "| **SGDRegressor** | $y = Xw + \\varepsilon$ | $\\mathcal{L}(w)=\\frac{1}{2N}\\|y - Xw\\|^2$ <br>+ (Regularización Adicional) | $w_{new} \\leftarrow w_{old} - \\eta \\nabla_w J_i(w), \\qquad$ <br>con $J_i(w)$ como el costo en una muestra <br>o mini-lote | Modelo de Mínimos Cuadrados <br>Regularizados por Norma L1 y L2 |Alta escalabilidad, algoritmo paralelizable e iterativo. <br>**SGD** es estándar industrial |\n",
        "| **BayesianRidge** | $y = Xw + \\varepsilon,$ <br>$\\quad w \\sim \\mathcal{N}(0, \\lambda^{-1}I),$ <br> $\\quad \\varepsilon \\sim \\mathcal{N}(0, \\beta^{-1}I)$ | $\\mathcal{L}(w) = \\frac{\\beta}{2}\\|y - Xw\\|^2 + \\frac{\\lambda}{2}\\|w\\|^2\n",
        "$ | $w_{\\text{MAP}} = (\\lambda I + \\beta X^\\top X)^{-1} (\\beta X^\\top y)$ | Regresión por Estimación a <br>Máximo Posteriori | Limitado por $(O(N^3))$, <br>debido a inversión de matrices |\n",
        "| **GaussianProcessRegressor** | $\n",
        "y = f(x) + \\varepsilon,$ <br>$f(x)\\sim \\mathcal{GP}(0, k(x,x')),$ <br>$\\qquad\\varepsilon \\sim \\mathcal{N}(0,\\sigma_n^2I)$ | $ \\qquad-\\log p(y \\mid X, \\theta) = \\qquad \\frac{1}{2}y^\\top (K + \\sigma_n^2 I)^{-1}y \\qquad + \\frac{1}{2}\\log|K + \\sigma_n^2 I|$ | Optimización iterativa de hiperparámetros $\\theta$ <br>mediante **gradiente descendente**, <br>implica la creación de una matriz K de tamaño NxN | Regresión Rígida por <br>Procesos Gaussianos | Limitado por $(O(N^3))$, <br>recreación e inversión de matriz kernel |\n",
        "| **SVR** | $\n",
        "f(x) = w^\\top \\phi(x) + b$ | $\\mathcal{L}=\\frac{1}{2}\\|w\\|^2+\\sum_{i=1}^{N}(\\xi_i + \\xi_i^*),$ <br>con $\\xi_i,\\xi_i^*$ que definen un margen de error<br> desde la línea de regresión | Resolución mediante <br>**Sequential Minimal Optimization (SMO)** <br>o **algoritmos QP** iterativos. | Modelo por Mínimos Cuadrados <br>y Regresión Rígida Kernel | Baja escalabilidad para N grandes, <br>costo computacional entre $(O(N^2))$ y $(O(N^3))$ |\n",
        "| **RandomForestRegressor** | $\n",
        "\\hat{y}(x) = \\frac{1}{T}\\sum_{t=1}^{T} h_t(x,\\Theta_t),$ $\\quad$ <br>con cada $h_t$ un árbol <br>entrenado en una submuestra | $\n",
        "\\mathcal{L(h_t)} = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y}_t(x_i))^2\n",
        "$ | Entrenamiento paralelo <br>de múltiples árboles independientes <br>usando **bagging** y búsqueda **greedy** <br>para minimizar la impureza de nodos | Ensamble de Modelos <br>por Mínimos Cuadrados <br>No Paramétrico | Alta escalabilidad, <br>algoritmo paralelizable e iterativo |\n",
        "| **GradientBoostingRegressor** | $\n",
        "\\hat{y}_m(x) = \\hat{y}_{m-1}(x) + \\nu \\, h_m(x)$| $J = \\sum_{i=1}^{N} \\mathcal{L}(y_i, \\hat{y}_m(x_i)), \\quad$ <br>con $\n",
        "\\mathcal{L}^{(t)} \\approx\n",
        "\\sum_i \\big[g_i f_t(x_i) + \\tfrac{1}{2} h_i f_t(x_i)^2 \\big]\n",
        "+ \\Omega(f_t)\n",
        "$| Ajuste secuencial de **árboles base** <br>sobre el **gradiente negativo** <br>de la función de pérdida | Ensamble de Modelos <br>por Mínimos Cuadrados <br>No Paramétrico | Alta escalabilidad, <br>algoritmo paralelizable e iterativo |\n",
        "| **XGBRegressor** | $\n",
        "\\hat{y}_t(x) = \\sum_{k=1}^{K} f_k(x), \\quad f_k \\in \\mathcal{F} \\text{ (espacio de árboles)}$ | $\\mathcal{L}^{(t)} = \\sum_{i=1}^{N} \\mathcal{L}(y_i, \\hat{y}_i^{(t-1)}) + f_t(x_i))+\\Omega(f_t),$ $\\qquad$ <br>con  $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\|w\\|^2\n",
        "$ | Optimización **iterativa** por **gradiente y hessiano**, <br>con búsqueda de divisiones óptimas en cada árbol. | Ensamble de Modelos <br>por Mínimos Cuadrados <br>No Paramétrico | Alta escalabilidad, <br>algoritmo paralelizable e iterativo |\n"
      ],
      "id": "czPQCoaztvaf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fADbshSWtvag"
      },
      "source": [
        "---\n",
        "\n",
        "# **Punto 3 - Introducción a RAPIDS**\n",
        "\n",
        "**RAPIDS** es un ecosistema de código abierto de NVIDIA diseñado para acelerar tareas de *data science* y *machine learning* utilizando GPUs.\n",
        "\n",
        "###**Componentes principales**\n",
        "- **cuDF:** Homólogo de Pandas con ejecución en GPU, hereda la mayoría de funcionalidades básicas de Pandas.\n",
        "- **cuML:** Modelos de Machine Learning comunes y no neuronales, son acelerados mediante GPU, estos modelos están basados en las implementaciones de scikit-learn, DBSCAN y XGBoost. Siendo estos dos últimos, ejecutados en GPU mediante compatibilidad con las librerías originales.\n",
        "- **cuPy:** Homólogo de NumPy con ejecución en GPU, así mismo, hereda las funcionalidades más usadas e importantes de NumPy original.\n",
        "\n",
        "Los modelos en cuML conservan la misma sintaxis `fit()`, `predict()` y `score()` que scikit-learn, permitiendo migraciones sencillas.\n",
        "\n",
        "###**Alternativa con Implementación Original**\n",
        "RAPIDS provee de aceleradores de código nativos en Pandas, NumPy y scikit-learn; es decir, es posible hacer uso de múltiples métodos e implementaciones de estas librerías originales sin migración de código a las librerías de RAPIDS. Estos aceleradores funcionan de manera que si el método llamado es compatible con ellos en el instante, la llamada de cómputo a CPU es intervenida, modificada y redireccionada para una posterior ejecución en la GPU disponible.\n",
        "\n",
        "Para activar estos aceleradores, se deben instalar inicialmente las librerías en cuestión y activar sus aceleradores mediante las siguientes líneas de código para pandas y scikit-learn:\n",
        "\n",
        "`%load_ext cudf.pandas`\n",
        "`%load_ext cuml.accel`\n",
        "\n",
        "En el siguiente link, se presenta una demo desarrollada por el equipo de RAPIDS, mostrando las mejoras en tiempo de cómputo entre el procedimiento con CPU nativo y el cambio frente a la vinculación del acelerador para tareas de ML con scikit-learn: https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/getting_started_tutorials/cuml_sklearn_colab_demo.ipynb"
      ],
      "id": "fADbshSWtvag"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTybum8Mtvag"
      },
      "source": [
        "## **Equivalencias entre Scikit-Learn y RAPIDS cuML**\n",
        "\n",
        "| Modelo (Scikit-Learn) | Implementación RAPIDS/cuML | Hiperparámetros Principales | Métodos | Atributos |\n",
        "|-----------------------|---------------------------|-----------------------------|--------------| ------------ |\n",
        "| **LinearRegression** | `cuml.LinearRegression()` | `algorithm` | `fit(X, y)` | `coef_`, `intercept_` |\n",
        "| **Lasso** | `cuml.Lasso()` | `alpha`, `solver` | `fit(X, y)` | `coef_`, `intercept_` |\n",
        "| **ElasticNet** | `cuml.ElasticNet()` | `alpha`, `l1_ratio` | `fit(X, y)`, `predict(X)` | `coef_`, `intercept_` |\n",
        "| **KernelRidge** | `cuml.KernelRidge()` | `alpha`, `kernel`, `gamma`, `degree`, `kernel_params` | `fit(X, y)`, `predict(X)` | `dual_coef_`, `X_fit_` |\n",
        "| **SGDRegressor** | `cuml.MBSGDRegressor()`| `loss`, `penalty`, `alpha`, `l1_ratio`, `batch_size`, `learning_rate` | `fit(X, y)`, `predict(X)` | `coef_`, `intercept_` |\n",
        "| **BayesianRidge** | No tiene implementación en cuML | `alpha_1`, `lambda_1`, `alpha_2`, `lambda_2`, `alpha_init`, `lambda_init`| `fit(X, y)`, `predict(X)` | `coef_`, `intercept_`, `alpha_`, `lambda_`, `sigma_` |\n",
        "| **GaussianProcessRegressor** | No tiene implementación en cuML | `alpha`, `kernel`, `optimizer` | `fit(X, y)`, `predict(X)`, `score(X, y)` | `kernel_`, `alpha_` |\n",
        "| **SVR** | `cuml.svm.SVR()` | `C`, `kernel`, `gamma`, `epsilon` | `fit(X, y)`, `predict(X)` | `coef_`, `intercept_` |\n",
        "| **RandomForestRegressor** | `cuml.ensemble.RandomForestRegressor()` | `n_estimators`, `max_depth`, `max_features`, `min_samples_leaf` | `fit(X, y)`, `predict(X)`, `score(X, y)` | Ninguno especificado en USerGuide |\n",
        "| **GradientBoosting/XGBoost** | `xgboost.XGBRegressor(tree_method='gpu_hist')` | `min_child_weight`, `max_depth`, `n_estimators`, `subsample` | `fit(X, y)`, `predict(X)` | Ninguno especificado en USerGuide |"
      ],
      "id": "BTybum8Mtvag"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P3ketQgtvah"
      },
      "source": [
        "---\n",
        "- En general, `alpha` y `l1_ratio` representan la magnitud y la proporción del coeficiente de regularización de los modelos en cuestión, respectivamente.\n",
        "- `solver`, `optimizer`, `algorithm`, `kernel`, `loss` y `penalty` serán parámetros tipo string que indican el tipo de cada parámetro asociado al modelo en cuestión.\n",
        "- Para algoritmos de árboles, tenemos que los parámetros de RFR: `max_depth`, `max_features` y `min_samples_leaf` son homólogos de los parámetros de XGBRegressor: `max_depth`, `subsample` y `min_child_weight`. `n_estimators` son distintos para cada modelo; en RFR son el número máximo de árboles que serán creados, mientras que en XGBR son el número de iteraciones y optimización a lo largo de todo el dataset, de los árboles en el modelo.\n"
      ],
      "id": "2P3ketQgtvah"
    }
  ]
}